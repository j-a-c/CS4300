{
 "metadata": {
  "name": "",
  "signature": "sha256:37b5072caef18514c7a076e9758058563971e3b990050c074a2befc183a63e54"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Assignment 02: \"Search your transcripts. You will know it to be true.\"\n",
      "## CS/INFO 4300 Language and Information\n",
      "\n",
      "**Due Tuesday, February 24 at 5pm**\n",
      "\n",
      "This assignment can be completed in groups of 1 to 2 (see description).\n",
      "\n",
      "In this assignment we will explore the tradeoffs of information retrieval systems by finding newspaper quotes from \"Keeping Up With The Kardashians\".\n",
      "\n",
      "**Guidelines**\n",
      "\n",
      "For code completion tasks, just type your code after the comment marking the place.  For questions, use as many notebook cells as needed to compute intermediate stuff. All floating point values should be printed with 2 decimal places precision.\n",
      "\n",
      "You are strongly encouraged to write sensible **test cases** for your code.\n",
      "\n",
      "# Setup\n",
      "\n",
      "Tabloids have been going crazy over our stars.  The press took some  quotes from the show, including:\n",
      "       \n",
      " - *\"It's like a bunch of people running around talking about nothing.\"*\n",
      " - *\"Never say to a famous person that this possible endorsment would bring 'er to the spot light.\"*\n",
      " - *\"Your yapping is making my head ache!\"*\n",
      " - *\"I'm going to Maryland, did I tell you?\"*\n",
      " \n",
      "We need to find out who said each of these, and in which episode. But since we're information scientists, that's not enough. We want to build an efficient search engine for retrieving where such quotes come from in the future.\n",
      "\n",
      "What makes this difficult is that journalists often modify the quotes, so exact matching will not always work."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from __future__ import print_function\n",
      "import numpy as np\n",
      "from collections import defaultdict\n",
      "from nltk.tokenize import TreebankWordTokenizer\n",
      "import Levenshtein  # package python-Levenshtein"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "queries = [u\"It's like a bunch of people running around talking about nothing.\",\n",
      "           u\"Never say to a famous person that this possible endorsment would bring 'er to the spot light.\",\n",
      "           u\"Your yapping is making my head ache!\",\n",
      "           u\"I'm going to Maryland, did I tell you?\"]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Load the data\n",
      "\n",
      "Load the transcripts provided in the `kardashian-transcripts.json` file."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import json\n",
      "with open(\"kardashian-transcripts.json\", \"rb\") as f:\n",
      "    transcripts = json.load(f)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Reorganize the data\n",
      "\n",
      "For this assignment, we'll consider documents to be individual message lines. The provided transcripts are grouped differently. Reorganize the data as a list of messages, where the messages are dictionary structures as provided."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "flat_msgs = [m for transcript in transcripts for m in transcript]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Data structure test\n",
      "msg = flat_msgs[0]\n",
      "print(msg)\n",
      "print('===')\n",
      "print(msg['text'])\n",
      "print('===')\n",
      "for tok in msg['toks']:\n",
      "    print(tok)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{u'toks': [u'you', u'do', u\"n't\", u'have', u'your', u'own', u'credit', u'card', u'?'], u'timestamp': u'00:00:23', u'episode_title': u'Keeping Up With the Kardashians - Shape Up or Ship Out', u'transcript_id': u'kardashians/153890', u'speaker': u'KHLOE', u'text': u\"You don't have your own credit card?\"}\n",
        "===\n",
        "You don't have your own credit card?\n",
        "===\n",
        "you\n",
        "do\n",
        "n't\n",
        "have\n",
        "your\n",
        "own\n",
        "credit\n",
        "card\n",
        "?\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Searching the collection\n",
      "\n",
      "The first and easiest thing to try is to directly compare the newspaper quote to the transcript strings.  If the press just copy-pasted from the transcript website, this might work.\n",
      "\n",
      "## Find all messages that include the given quotes exactly.\n",
      "\n",
      "Print the episode title, speaker name and full message, for all messages that exactly contain a given quote.\n",
      "\n",
      "Write this as a function `verbatim_search` and run the function for each of the 4 quotes."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Q1 Write a function `verbatim_search` that looks for exact matches of a query in each message.\n",
      "\n",
      "Use `in`: `'efg' in 'cdefgh'` is `True`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def verbatim_search(query, msgs):\n",
      "    \"\"\" Verbatim search\n",
      "    \n",
      "    Arguments\n",
      "    =========\n",
      "    \n",
      "    query: string,\n",
      "        The query we are looking for.\n",
      "        \n",
      "    msgs: list of dicts,\n",
      "        Each message in this list has a 'text' field with\n",
      "        the raw document.\n",
      "    \n",
      "    Returns\n",
      "    =======\n",
      "    result: list of messages\n",
      "        All messages that exactly contain the query string.\n",
      "    \n",
      "    \"\"\"\n",
      "    \n",
      "    matching_msgs = []\n",
      "    for msg in msgs:\n",
      "        if query in msg['text']:\n",
      "            matching_msgs.append(msg)\n",
      "            \n",
      "    return matching_msgs\n",
      "\n",
      "\n",
      "def verbatim_search_test1():\n",
      "    msg1 = {'text':'cdefg'}\n",
      "    msg2 = {'text':'cdfg'}\n",
      "    msg3 = {'text':'asdf?'}\n",
      "    msgs = [msg1, msg2, msg3]\n",
      "    \n",
      "    matching_queries = verbatim_search('efg', msgs)\n",
      "    \n",
      "    assert(len(matching_queries) == 1)\n",
      "    assert(matching_queries[0]['text'] == 'cdefg')\n",
      "    \n",
      "    print('verbatim_search_test1 successful')\n",
      "    \n",
      "verbatim_search_test1()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "verbatim_search_test1 successful\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for query in queries:\n",
      "    print(query)\n",
      "    print(\"===\")\n",
      "    for msg in verbatim_search(query, flat_msgs):\n",
      "        print(\"{}: {}\\n\\t({})\\n\".format(msg['speaker'].encode('utf-8'),\n",
      "                                        msg['text'].encode('utf-8'),\n",
      "                                        msg['episode_title'].encode('utf-8')))\n",
      "    print()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "It's like a bunch of people running around talking about nothing.\n",
        "===\n",
        "BRUCE: It's like a bunch of people running around talking about nothing.\n",
        "\t(Keeping Up With the Kardashians - Kourt's First Cover)\n",
        "\n",
        "\n",
        "Never say to a famous person that this possible endorsment would bring 'er to the spot light.\n",
        "===\n",
        "\n",
        "Your yapping is making my head ache!\n",
        "===\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "I'm going to Maryland, did I tell you?\n",
        "===\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Find the most similar messages (cosine similarity)\n",
      "\n",
      "We will use an **inverted index** for efficiency. This is a sparse term-centered representation that allows us to quickly find all documents that contain a given term.\n",
      "\n",
      "### Q2 Write a function to construct the index.\n",
      "\n",
      "As in class, the index is a key-value structure where the keys are terms and the values are lists of *postings*. In this case, like in class, we record the documents a term occurs in as well as the **count** of that term in that document."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def build_inverted_index(msgs):\n",
      "    \"\"\" Builds an inverted index from the messages.\n",
      "    \n",
      "    Arguments\n",
      "    =========\n",
      "    \n",
      "    msgs: list of dicts.\n",
      "        Each message in this list already has a 'toks'\n",
      "        field that contains the tokenized message.\n",
      "    \n",
      "    Returns\n",
      "    =======\n",
      "    \n",
      "    index: dict\n",
      "        For each term, the index contains a list of\n",
      "        tuples (doc_id, count_of_term_in_doc):\n",
      "        index[term] = [(d1, tf1), (d2, tf2), ...]\n",
      "        \n",
      "    Example\n",
      "    =======\n",
      "    \n",
      "    >> test_idx = build_inverted_index([\n",
      "    ...    {'toks': ['to', 'be', 'or', 'not', 'to', 'be']},\n",
      "    ...    {'toks': ['do', 'be', 'do', 'be', 'do']}])\n",
      "    \n",
      "    >> test_idx['be']\n",
      "    [(0, 2), (1, 2)]\n",
      "    \n",
      "    >> test_idx['not']\n",
      "    [(0, 1)]\n",
      "    \n",
      "    \"\"\"\n",
      "    inverted_index = defaultdict(list)\n",
      "    \n",
      "    next_id = 0\n",
      "    \n",
      "    for msg in msgs:\n",
      "        doc_id = next_id\n",
      "        tokens = msg['toks']\n",
      "        for token in set(tokens):\n",
      "            tok_list = inverted_index[token]\n",
      "            tok_list.append((doc_id, tokens.count(token)))\n",
      "        next_id += 1\n",
      "    \n",
      "    return inverted_index\n",
      "\n",
      "\n",
      "def build_inverted_index_test1():\n",
      "    test_idx = build_inverted_index([\n",
      "         {'toks': ['to', 'be', 'or', 'not', 'to', 'be']},\n",
      "        {'toks': ['do', 'be', 'do', 'be', 'do']}])\n",
      "    \n",
      "    assert(test_idx['be'] == [(0, 2), (1, 2)])\n",
      "    assert(test_idx['not'] == [(0, 1)])\n",
      "    \n",
      "    print('build_inverted_index_test1 successful')\n",
      "    \n",
      "build_inverted_index_test1()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "build_inverted_index_test1 successful\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Q3 Compute IDF *using* the inverted index\n",
      "\n",
      "Write a function `compute_idf` that uses the inverted index to efficiently compute IDF values.\n",
      "\n",
      "Words that occur in a very small number of documents are not useful in many cases, so we ignore them. Use a parameter `min_df`\n",
      "to ignore all terms that occur in strictly fewer than `min_df` documents.\n",
      "\n",
      "Similarly, words that occur in a large *fraction* of the documents don't bring any more information for some tasks. Use a parameter `max_df_ratio` to trim out such words. For example, `max_df_ratio=0.95` means ignore all words that occur in more than 95% of the documents."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%latex\n",
      "\n",
      "$$ IDF(t) = \\log \\left(\\frac{N}{1 + DF(t)} \\right) $$\n",
      "\n",
      "N = total number of docs; \n",
      "\n",
      "$DF(t)$ = number of docs containing $t$."
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "latex": [
        "\n",
        "$$ IDF(t) = \\log \\left(\\frac{N}{1 + DF(t)} \\right) $$\n",
        "\n",
        "N = total number of docs; \n",
        "\n",
        "$DF(t)$ = number of docs containing $t$."
       ],
       "metadata": {},
       "output_type": "display_data",
       "text": [
        "<IPython.core.display.Latex at 0xeb1b62c>"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def compute_idf(inv_idx, n_docs, min_df=10, max_df_ratio=0.95):\n",
      "    \"\"\" Compute term IDF values from the inverted index.\n",
      "    \n",
      "    Words that are too frequent or too infrequent get pruned.\n",
      "    \n",
      "    \n",
      "    Arguments\n",
      "    =========\n",
      "    \n",
      "    inv_idx: an inverted index as above\n",
      "    \n",
      "    n_docs: int,\n",
      "        The number of documents.\n",
      "        \n",
      "    min_df: int,\n",
      "        Minimum number of documents a term must occur in.\n",
      "        Less frequent words get ignored.\n",
      "    \n",
      "    max_df_ratio: float,\n",
      "        Maximum ratio of documents a term can occur in.\n",
      "        More frequent words get ignored.\n",
      "    \n",
      "    Returns\n",
      "    =======\n",
      "    \n",
      "    idf: dict\n",
      "        For each term, the dict contains the idf value.\n",
      "        \n",
      "    \"\"\"\n",
      "    idf = {}\n",
      "    \n",
      "    max_cut_off = 0.95 * n_docs\n",
      "    \n",
      "    for term in inv_idx:\n",
      "        doc_list = inv_idx[term]\n",
      "        num_docs_with_term = len(doc_list)\n",
      "        \n",
      "        # Apply thresholding\n",
      "        if (num_docs_with_term >= min_df) and (num_docs_with_term <= max_cut_off):\n",
      "            idf[term] = np.log(n_docs / (1.0+num_docs_with_term))\n",
      "            \n",
      "    return idf\n",
      "\n",
      "def compute_idf_test1():\n",
      "    test_idx = build_inverted_index([\n",
      "         {'toks': ['to', 'be', 'or', 'not', 'to', 'be']},\n",
      "        {'toks': ['do', 'be', 'do', 'be', 'do']},\n",
      "        {'toks': ['asdf', 'fsda', 'be']}])\n",
      "    \n",
      "    idf = compute_idf(test_idx, 3, 0, max_df_ratio=0.95)\n",
      "    \n",
      "    assert('be' not in idf)\n",
      "    assert(idf['do'] == np.log(3/2.0))\n",
      "    \n",
      "    print('compute_idf_test1 successful.')\n",
      "    \n",
      "compute_idf_test1()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "compute_idf_test1 successful.\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Q4 Compute the norm of each document using the inverted index"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%latex\n",
      "\n",
      "$$ \\left|\\left| d_j \\right|\\right| = \\sqrt{\\sum_i (tf_{ij} \\cdot idf_i)^2} $$"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "latex": [
        "\n",
        "$$ \\left|\\left| d_j \\right|\\right| = \\sqrt{\\sum_i (tf_{ij} \\cdot idf_i)^2} $$"
       ],
       "metadata": {},
       "output_type": "display_data",
       "text": [
        "<IPython.core.display.Latex at 0x106097d90>"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import math\n",
      "\n",
      "def compute_doc_norms(index, idf, n_docs):\n",
      "    \"\"\" Precompute the euclidean norm of each document.\n",
      "    \n",
      "    Arguments\n",
      "    =========\n",
      "    \n",
      "    index: the inverted index as above\n",
      "    \n",
      "    idf: dict,\n",
      "        Precomputed idf values for the terms.\n",
      "    \n",
      "    n_docs: int,\n",
      "        The total number of documents.\n",
      "    \n",
      "    Returns\n",
      "    =======\n",
      "    \n",
      "    norms: list of np.array, size: n_docs\n",
      "        norms[i] = the norm of document i.\n",
      "    \"\"\"\n",
      "    \n",
      "    norms = [0.0 for i in range(n_docs)]\n",
      "    \n",
      "    for term in idf:\n",
      "        term_idf = idf[term]\n",
      "        \n",
      "        for doc_freq_pair in index[term]:\n",
      "            doc_id = doc_freq_pair[0]\n",
      "            doc_freq = doc_freq_pair[1]\n",
      "            \n",
      "            norms[doc_id] += (doc_freq * term_idf)**2\n",
      "            \n",
      "    norms = [math.sqrt(i) for i in norms]\n",
      "    return norms\n",
      "\n",
      "\n",
      "def compute_doc_norms_test1():\n",
      "    \n",
      "    test_docs = [\n",
      "         {'toks': ['to', 'be', 'or', 'not', 'to', 'be']},\n",
      "        {'toks': ['do', 'be', 'do', 'be', 'do']},\n",
      "        {'toks': ['asdf', 'fsda', 'be']}]\n",
      "    \n",
      "    n_docs = 3\n",
      "    \n",
      "    test_idx = build_inverted_index(test_docs)\n",
      "    \n",
      "    idf = compute_idf(test_idx, n_docs, 0, max_df_ratio=0.95)\n",
      "    \n",
      "    norms = compute_doc_norms(test_idx, idf, n_docs)\n",
      "    \n",
      "    assert(len(norms) == n_docs)\n",
      "    \n",
      "    doc_2_norm = math.sqrt( (idf['asdf'])**2 + (idf['fsda'])**2 )\n",
      "    assert(norms[2] == doc_2_norm)\n",
      "    \n",
      "    print('compute_doc_norms_test1 successful.')\n",
      "    \n",
      "    \n",
      "compute_doc_norms_test1()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "compute_doc_norms_test1 successful.\n"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Q5 Find the most similar messages to the quotes.  Write the `search_index` function.\n",
      "\n",
      "Run the search using the code provided. Discuss why it worked, or why it might not have worked, for each query.\n",
      "\n",
      "Use cosine similarity. Assume the weight of any term in the query is 1, and the norm of the query is 1.\n",
      "\n",
      "**Note** Use the `nltk.tokenize.TreebankWordTokenizer` to tokenize the query. The transcripts have already been tokenized this way.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Aside:** Precomputation\n",
      "\n",
      "In many settings, we will need to repeat the same kind of operation many times. Often, part of the input doesn't change.\n",
      "Queries against the Kardashians transcript are like this: we want to run more queries (in the real world we'd want to run a lot of them every second, even) but the data we are searching doesn't change.\n",
      "\n",
      "We could write an `index_search` function with the same signature as `verbatim_search`, taking the `query` and the `msgs` as input, and the function would look like:\n",
      "\n",
      "    def index_search(query, msgs):\n",
      "        inv_idx = build_inverged_index(msgs)\n",
      "        idf = compute_idf(inv_idx, len(msgs))\n",
      "        doc_norms = compute_doc_norms(inv_idx)\n",
      "        # do actual search\n",
      "\n",
      "\n",
      "But notice that the first three lines only depend on the messages. Imagine if we run this a million times with different queries but the same collection of documents: we'd wastefully recompute the index, the IDFs and the norms every time and discard them.  It's a better idea, then, to precompute them just once, and pass them as arguments."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "inv_idx = build_inverted_index(flat_msgs)\n",
      "\n",
      "idf = compute_idf(inv_idx, len(flat_msgs),\n",
      "                  min_df=10,\n",
      "                  max_df_ratio=0.1)  # documents are very short so we can use a small value here\n",
      "                                     # examine the actual DF values of common words like \"the\"\n",
      "                                     # to set these values\n",
      "\n",
      "inv_idx = {key: val for key, val in inv_idx.items()\n",
      "           if key in idf}            # prune the terms left out by idf\n",
      "\n",
      "doc_norms = compute_doc_norms(inv_idx, idf, len(flat_msgs))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tokenizer = TreebankWordTokenizer()\n",
      "tokenizer.tokenize(\"a more general-purpose tokenizer\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def index_search(query, index, idf, doc_norms):\n",
      "    \"\"\" Search the collection of documents for the given query\n",
      "    \n",
      "    Arguments\n",
      "    =========\n",
      "    \n",
      "    query: string,\n",
      "        The query we are looking for.\n",
      "    \n",
      "    index: an inverted index as above\n",
      "    \n",
      "    idf: idf values precomputed as above\n",
      "    \n",
      "    doc_norms: document norms as computed above\n",
      "    \n",
      "    Returns\n",
      "    =======\n",
      "    \n",
      "    results, list of tuples (score, doc_id)\n",
      "        Sorted list of results such that the first element has\n",
      "        the highest score, and `doc_id` points to the document\n",
      "        with the highest score.\n",
      "        \n",
      "    \"\"\"\n",
      "    pass"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for query in queries:\n",
      "    print(\"#\" * len(query))\n",
      "    print(query)\n",
      "    print(\"#\" * len(query))\n",
      "\n",
      "    for score, msg_id in index_search(query, inv_idx, idf, doc_norms)[:10]:\n",
      "        print(\"[{:.2f}] {}: {}\\n\\t({})\".format(\n",
      "            score,\n",
      "            flat_msgs[msg_id]['speaker'].encode('utf-8'),\n",
      "            flat_msgs[msg_id]['text'].encode('utf-8'),\n",
      "            flat_msgs[msg_id]['episode_title'].encode('utf-8'))) \n",
      "    print()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Find the most similar messages to the quotes in terms of Edit Distance\n",
      "\n",
      "This section is most easily solved using the [python-Levenshtein](https://github.com/ztane/python-Levenshtein) package.\n",
      "\n",
      "### Q6 Write an `edit_distance_search` function.\n",
      "\n",
      "This time, do not use a tokenizer, as we are looking for character-level edits of the quotes, to catch typos and variations.\n",
      "Run the search using the code provided. Discuss why it worked, or why it might not have worked, for each query."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def _edit(query_str, msg):\n",
      "    return Levenshtein.distance(query_str.lower(), msg['text'].lower())\n",
      "\n",
      "\n",
      "def edit_distance_search(query, msgs):\n",
      "    \"\"\" Edit distance search\n",
      "    \n",
      "    Arguments\n",
      "    =========\n",
      "    \n",
      "    query: string,\n",
      "        The query we are looking for.\n",
      "        \n",
      "    msgs: list of dicts,\n",
      "        Each message in this list already has a 'toks'\n",
      "        field that contains the tokenized message.\n",
      "    \n",
      "    Returns\n",
      "    =======\n",
      "    \n",
      "    result: list of (score, message) tuples.\n",
      "        The result list is sorted by score such that the closest match\n",
      "        is the top result in the list.\n",
      "    \n",
      "    \"\"\"\n",
      "    pass"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for query in queries:\n",
      "    print(\"#\" * len(query))\n",
      "    print(query)\n",
      "    print(\"#\" * len(query))\n",
      "\n",
      "    for score, msg in edit_distance_search(query, flat_msgs)[:10]:\n",
      "        print(\"[{:.2f}] {}: {}\\n\\t({})\".format(\n",
      "            score,\n",
      "            msg['speaker'].encode('utf-8'),\n",
      "            msg['text'].encode('utf-8'),\n",
      "            msg['episode_title'].encode('utf-8')))\n",
      "    print()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Q7 Print the changes that need to be done to each quote to make it look like the closest match.\n",
      "\n",
      "Most of the code needed is provided below."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "a = \"kardashians\"\n",
      "b = \"dalmatians\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def print_edits(str_a, str_b, edits):\n",
      "    output = [[char] for char in str_a]\n",
      "    indices = np.arange(len(str_a) + 1)\n",
      "    for op, src, dest in edits:\n",
      "        if op == 'insert':\n",
      "            src = indices[src]\n",
      "            output.insert(src, [\"<span class='add'>{}</span>\".format(str_b[dest])])\n",
      "            indices += 1\n",
      "        elif op == 'replace':\n",
      "            src = indices[src]\n",
      "            src_char = output[src][0]\n",
      "            output[src] = output[src][1:]\n",
      "            output[src].append(\"<span class='del'>{}</span><span class='add'>{}</span>\".format(src_char, str_b[dest]))\n",
      "        elif op == 'delete':\n",
      "            src = indices[src]\n",
      "            src_char = output[src].pop()\n",
      "            output[src].append(\"<span class='del'>{}</span>\".format(src_char))\n",
      "    \n",
      "    return \"<div class='edit'>{}</div>\".format(\"\".join(\"\".join(stack) for stack in output))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.display import HTML"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "HTML(\"\"\"\n",
      "<style type=\"text/css\">\n",
      "\n",
      ".edit {font-size: 20px;}\n",
      ".del {text-decoration: line-through; color: #aaa;}\n",
      ".add {color: green; font-weight: bold;}\n",
      "</style>\n",
      "\"\"\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "edits = Levenshtein.editops(a, b)\n",
      "HTML(print_edits(a, b, edits))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Bonus question for extra course credit.\n",
      "\n",
      "### Updating precomputed values.\n",
      "\n",
      "In many real-world applications, the collection of documents will not stay the same forever. At Internet-scale, however, it could possibly even be worth recomputing things every second, if during that second we're going to answer millions of queries.\n",
      "\n",
      "However, there's a better way: in reality, the document set will not change radically, but incrementally.  In particular, it's most common to add or remove a bunch of new documents to the index.\n",
      "\n",
      "Write functions `add_docs` and `remove_docs` that update the index, idf and document norms.  Think of the implications this has on how we store the IDF. Is there a better way of storing it, that minimizes the memory we need to touch when updating?\n",
      "\n",
      "Think of adequate test cases for these functions and implement them."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}