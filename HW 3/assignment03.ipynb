{
 "metadata": {
  "name": "",
  "signature": "sha256:c12e391627427cecf0150f638a5c70d9c28bfb7fd851f2b38a18904e72a82fd1"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Assignment 03: \"Tragedy Tomorrow, Comedy Tonight\"\n",
      "## CS/INFO 4300 Language and Information\n",
      "\n",
      "\n",
      "In class we have been discussing text classification methods for problems such as speaker attribution. For this assignment we are going to look at text features to in the context of classifying movies into genres based on the text of the script. Many of the\n",
      "questions rely on reading the [scikit-learn\n",
      "documentation](http://scikit-learn.org/stable/).\n",
      "\n",
      "The assignment focuses on vectorizers, support vector machine classification,\n",
      "multi-label evaluation, cross-validation and grid search.\n",
      "\n",
      "\n",
      "**Due Friday, April 10 at 5pm** \n",
      "\n",
      "This assignment can be completed in groups of 2 to 3 (see description).\n",
      "\n",
      "\n",
      "**Guidelines**\n",
      "\n",
      "For code completion tasks, just type your code after the comment marking the place.  For questions, use as many notebook cells as needed to compute intermediate stuff. All floating point values should be printed with 2 decimal places precision. We have included all of the necessary imports.\n",
      "\n",
      "You are strongly encouraged to write sensible test cases for your code, especially when suggested ones are already provided in the description.\n",
      "\n",
      "We have included the code used to create the data.json file. Remember it is always a good idea to look at the original data before you start working.\n",
      "\n",
      "# Setup"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import json"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Q01. Load the data from the JSON file.\n",
      "\n",
      "Build a numpy array `movie_text` with the script contents and a numpy array `movie_cat` with the categories for each movie.\n",
      "If you preserve the order, you should get:\n",
      "\n",
      "    movie_text[0][:30] = u'He was the first man among us.'\n",
      "\n",
      "    movie_cat[0] = ['comedy', 'drama', 'romance']\n",
      "    movie_cat[11] = ['action', 'adventure', 'mystery', 'thriller']"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "movie_data = json.load(open('movie_scripts_data.json'))\n",
      "\n",
      "movie_text = []\n",
      "movie_cat = []\n",
      "\n",
      "for movie in movie_data:\n",
      "    movie_text.append(movie['script'])\n",
      "    movie_cat.append(movie['categories'])\n",
      "\n",
      "def test_json_import():\n",
      "    assert movie_text[0][:30] == u'He was the first man among us.'\n",
      "    \n",
      "    assert movie_cat[0] == ['comedy', 'drama', 'romance']\n",
      "    assert movie_cat[11] == ['action', 'adventure', 'mystery', 'thriller']\n",
      "    \n",
      "    \n",
      "test_json_import()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Q02. Binarize the category labels.\n",
      "\n",
      "Most machine learning problems you probably faced are either binary (e.g. spam or no spam), or multiclass (e.g. which of the kardashians is speaking?).  You might have noticed that our current problem is slightly different: the categories are not mutually exclusive.  A movie can belong to multiple categories (e.g. a romantic comedy has both *romance* and *comedy*), while it's possible for a movie not to belong to any categories (in case the set of categories we consider is incomplete). This kind of scenario is called \"multi-label classification.\"\n",
      "\n",
      "In this assignment we will see that there is a simple way to deal with multi-label learning.\n",
      "\n",
      "First, we must transform the label information to a machine-friendly format. Use `sklearn.preprocessing.MultiLabelBinarizer` to transform `movie_cat` into a binary matrix of the same length, with a column corresponding to each category. Call the resulting matrix `Y`. If you instantiate the `MultiLabelBinarizer` as `mlb`, you should get the following output, which matches the above:\n",
      "\n",
      "    Y.shape == (617, 24)\n",
      "    np.where(Y[11]) == (array([ 0,  2, 16, 21]),)\n",
      "    mlb.classes_[[0, 2, 16, 21]] == array(['action', 'adventure', 'mystery', 'thriller'], dtype=object)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.preprocessing import MultiLabelBinarizer\n",
      "\n",
      "mlb = MultiLabelBinarizer()\n",
      "Y = mlb.fit_transform(movie_cat)\n",
      "\n",
      "def test_mlb():\n",
      "    assert Y.shape == (617, 24)\n",
      "    assert str(np.where(Y[11])) == '(array([ 0,  2, 16, 21]),)'\n",
      "    assert all(mlb.classes_[[0,2,16,21]] == np.array(['action', 'adventure', 'mystery', 'thriller'], dtype=object))\n",
      "\n",
      "test_mlb()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Q03. Remove categories with 50 or fewer movies\n",
      "\n",
      "Reduce Y into a smaller matrix by keeping only the columns whose sum is at least 50.\n",
      "Build a `category_names` array that contains the names of the categories that are kept (a subset of `mlb.classes_`).\n",
      "\n",
      "You should get\n",
      "\n",
      "    category_names[:2] == array(['action', 'adventure'], dtype=object)\n",
      "    \n",
      "Note that after this, it's quite likely we'll be left with some movies with no labels assigned. This is alright."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "columns_to_keep = [col for col in range(Y.shape[1]) if np.sum(Y[:,col]) >= 50]\n",
      "\n",
      "category_names = mlb.classes_[columns_to_keep]\n",
      "Y = Y[:,columns_to_keep]\n",
      "\n",
      "def test_category_removal():\n",
      "    np.testing.assert_array_equal(category_names[:2], np.array(['action', 'adventure']))\n",
      "    \n",
      "    assert Y.shape == (617, len(columns_to_keep))\n",
      "\n",
      "test_category_removal()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Q04. Make a 50-50 train-test split.\n",
      "\n",
      "Use `sklearn.cross_validation.train_test_split`. Set `random_state=0`. Make sure the train and test sizes are equal (plus/minus one)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.cross_validation import train_test_split\n",
      "\n",
      "text_train, text_test, Y_train, Y_test = train_test_split(movie_text, Y, test_size=0.5, random_state=0)\n",
      "\n",
      "def test_split():\n",
      "    assert abs(text_train.shape[0] - text_test.shape[0]) <= 1\n",
      "    assert abs(Y_train.shape[0] - Y_test.shape[0]) <= 1\n",
      "    assert text_train.shape[0] == Y_train.shape[0]\n",
      "    assert text_test.shape[0] == Y_test.shape[0]\n",
      "    \n",
      "test_split()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Q05. Build the document-term matrices\n",
      "\n",
      "Use `sklearn.feature_extraction.TfidfVectorizer`. Use unigrams only, disable idf, use `l1` normalization. Ignore words that appear in fewer than 50 documents and in more than 80% of the documents. (All of the above can be achieved through arguments passed to the `TfidfVectorizer`.)\n",
      "\n",
      "Call the resulting matrices `X_train` and `X_test`.\n",
      "\n",
      "**Note:** Remember to just `fit` on the training data. If a word occurs only in the test documents, our model should **not** be aware that the word exists, as we are trying to evaluate the performance on completely unseen data."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "\n",
      "vectorizer = TfidfVectorizer(ngram_range=(1,1), use_idf=False, norm='l1', min_df=50, max_df=0.80)\n",
      "\n",
      "X_train = vectorizer.fit_transform(text_train)\n",
      "\n",
      "X_test = vectorizer.transform(text_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Q06. Predict using a random guess baseline\n",
      "\n",
      "Our whole endeavor wouldn't be justified if we couldn't reach better performance than just some intern flipping a coin and labeling movies accordingly.  So let's first check how well such a \"dumb\" predictor can perform.  To make life even harder for us, we can knock it up a notch: for each category, the probability of saying that a test document has it should be the observed frequency of that category in the training data. (Mathematically, we sample $y_c \\sim Bernoulli(P(c))$\n",
      "\n",
      "This approach is implemented in `sklearn.dummy.DummyClassifier`.  Make sure to set `strategy=\"stratified\"`, but read about the other dummy strategies available. Make sure to set `random_state=0`, to get the same result every time, since randomness is involved.\n",
      "\n",
      "Fit a `DummyClassifier` on the training data (use `dummy.fit`) and use it to make predictions for the test data and build `Y_pred_stratified`, which should have the same shape as `Y_test` (use `dummy.predict`)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.dummy import DummyClassifier\n",
      "\n",
      "dummy_classifier = DummyClassifier(strategy='stratified', random_state=0)\n",
      "\n",
      "dummy_classifier.fit(X_train, Y_train)\n",
      "\n",
      "Y_pred_stratified = dummy_classifier.predict(X_test)\n",
      "\n",
      "\n",
      "assert Y_pred_stratified.shape == Y_test.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Q07. Evaluate the randomized predictions\n",
      "\n",
      "We will use a multi-label evaluation metric called *sample F1 score*, which is closely related to the F1 score discussed in class.  If a document has the true labels \"comedy\" and \"sci-fi\", but the classifier predicts \"action\", \"sci-fi\" and \"adventure,\n",
      "then the sample precision is 1/3, the sample recall is 1/2, and the sample F1 is the harmonic mean of the two.\n",
      "\n",
      "In scikit-learn, sample F1 score for multi-label problems is implemented by `sklearn.metrics.f1_score` **with the argument average=\"samples\"**.\n",
      "\n",
      "You should get a not-so-impressive score.\n",
      "\n",
      "DISCUSSION ITEM.\n",
      "What would happen if you forget to set `average=\"samples\"`? How would this change our results?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.metrics import f1_score\n",
      "\n",
      "dummy_score = f1_score(Y_test, Y_pred_stratified, average='samples')\n",
      "print 'Dummy score', dummy_score\n",
      "\n",
      "print 'Score', f1_score(Y_test, Y_pred_stratified)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Dummy score 0.266775475513\n",
        "Score 0.297538685374\n"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "TODO If you forget to set average='samples'..."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Q08. Train and evaluate a classifier.\n",
      "\n",
      "We will use `sklearn.svm.LinearSVC()` as our building-block classifier. However, a `LinearSVC` is a binary classifier, while we need to predict multiple labels (a document can be comedy and sci-fi at the same time).\n",
      "\n",
      "To achieve this, we wrap the `LinearSVC` using `sklearn.multiclass.OneVsRestClassifier`, which will build for us a separate `LinearSVC` to recognize each category label.\n",
      "\n",
      "Name the `OneVsRestClassifier` `ovr` and train it on the training movies. **What shape does `ovr.coef_` have? Why?**\n",
      "\n",
      "Now, use it to make predictions on the test data and build `Y_pred_svm`.  Compute the *sample F1* score just as you did in Q06. You should see a slight but not huge improvement. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.svm import LinearSVC\n",
      "from sklearn.multiclass import OneVsRestClassifier\n",
      "\n",
      "ovr = OneVsRestClassifier(LinearSVC())\n",
      "\n",
      "\n",
      "ovr.fit(X_train, Y_train)\n",
      "\n",
      "Y_pred_svm = ovr.predict(X_test)\n",
      "\n",
      "ovr_score = f1_score(Y_test, Y_pred_svm, average='samples')\n",
      "\n",
      "print 'ovr f1 score:', ovr_score\n",
      "\n",
      "print 'ovr.coef_ shape:', ovr.coef_.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "ovr f1 score: 0.309570041609\n",
        "ovr.coef_ shape: (11, 1164)\n"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "TODO"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Q09. Use grid search and cross-validation to tune the classifier\n",
      "\n",
      "The score above is pretty disappointing, but kind of expected, given how little work we did-- we are basically just using the default configuration.  A `LinearSVC` has a bunch of configuration options that should be tweaked:\n",
      "\n",
      "* `C` is the *regularization parameter*. Lower values of C constraint the model more, while higher values allows the model to fit the training data better. (Remember that fitting the training data too well can lead to overfitting.)\n",
      "\n",
      "* `class_weight` can force the classifier to emphasize positive instances more or less than negative ones. This is useful if we know for a fact that the classes aren't equally probable. Read the documentation and see what the `'auto'` setting does.\n",
      "\n",
      "However, choosing these values should also be done without looking at the test data, because they are part of the model. Use `sklearn.grid_search.GridSearchCV` to systematically try out different values for these two parameters, and choose the configuration that does best.\n",
      "\n",
      "`GridSearchCV` uses k-fold cross-validation to ensure fair evaluation and avoid overfitting. This consists of splitting the training data into *k* parts, then training the classifier *k* times, each time leaving out a different part, that is used for scoring. The average score over the *k* folds is a better estimate of how well the classifier would generalize.\n",
      "\n",
      "Because we are facing a multi-label problem, the default scoring strategy (accuracy) doesn't make sense. We have to define our own `sample_f1_scorer` strategy:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def sample_f1_scorer(estimator, X, y):\n",
      "    \"\"\"sample-f1 scorer metric\n",
      "    \n",
      "    This function is just glue code for the scikit-learn scorer API.\n",
      "    See http://scikit-learn.org/stable/modules/model_evaluation.html#implementing-your-own-scoring-object\n",
      "    \n",
      "    Parameters:\n",
      "    -----------\n",
      "    \n",
      "    estimator:\n",
      "        the model that should be evaluated (e.g., the scikit-learn classifier)\n",
      "    X: array-like, shape (n_samples, n_features)\n",
      "        the test data\n",
      "    y: array-like, shape (n_samples, n_labels)\n",
      "        the ground truth target for X.\n",
      "    \n",
      "    Returns:\n",
      "    --------\n",
      "    \n",
      "    sample_f1_score, float\n",
      "        the sample F1 score as used in Q06 and Q07\n",
      "    \"\"\" \n",
      "    pass"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now, run grid search over a range of regularization parameters, as below.  This takes under 1 minute on a 2014 MacBook Pro Retina. If you're not sure your code works, test it on a small number of documents first to avoid wasting time.\n",
      "\n",
      "What is the best configuration, and the best score (averaged over the 3 folds)? (there are attributes of the `GridSearchCV` object that answer this).\n",
      "\n",
      "DISCUSSION ITEM.\n",
      "What can you say about the impact of `C` and `class_weight` on the score? (look at `grid.grid_scores_` to answer this)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.grid_search import GridSearchCV"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 43
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "param_grid = dict(\n",
      "    estimator__C=[1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3],  # you can also build this using np.logspace\n",
      "    estimator__class_weight=['auto', None])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 56
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "grid = GridSearchCV(ovr,\n",
      "                    param_grid,\n",
      "                    cv=3,\n",
      "                    scoring=sample_f1_scorer,\n",
      "                    verbose=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 72
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "TODO discussion"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Q10. Evaluate the chosen classifier on the test set. Inspect performance on individual categories.\n",
      "\n",
      "Use `grid.best_estimator_` to access the `ovr` object chosen as best by the grid search. Use `sample_f1_scorer` and report the **sample F1** score as in Q06 and Q07. This time, you should see a rewarding increase.\n",
      "\n",
      "DISCUSSION ITEM.\n",
      "Compare this score with the cross-validated average score over the 3 folds for the best model (Q08).  Does cross-validation give a reasonable estimate of the actual generalization performance a model can get on unseen test data? Compare with what we saw in class, when we were looking at the performance of a classifier on the data it was trained on, versus on the test data."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "Then, to aggregate scores over individual categories, use `sklearn.metrics.classification_report`. Keep in mind that in the classification report, precision, recall and F1 have different meaning than the sample-based scores we used in the previous questions: they are averages over a given label, as opposed to a given document.\n",
      "\n",
      "DISCUSSION ITEM. How do you interpret this table?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.metrics import classification_report"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 65
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Q11. What are the words most indicative of each category?\n",
      "\n",
      "Use `vectorizer.get_feature_names()` to get a list of all the words used, in the order that matches the columns of `X_train` and `X_test`. Use `grid.best_estimator_` to access the `ovr` object chosen as best by the grid search.  Use its `.coef_` attribute (remember the interpretation from Q07). Print the 10 words with the highest coefficients for each category.\n",
      "\n",
      "DISCUSSION ITEM. Discuss the top features. Do they suggest any changes that we could make to improve the model?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Bonus to get you thinking for your final project \n",
      "\n",
      "As we discussed in class there are different additional features that you can use for classification tasks. For bonus points try adding new features to see if you can improve classfication.  Please include an evaluation (under the same framework as above) and explanation of your features and why you chose to use them.\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}